{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":10,"metadata":{"id":"t8FpaltAw89L","executionInfo":{"status":"ok","timestamp":1734048711105,"user_tz":480,"elapsed":1241,"user":{"displayName":"Carson Chiem","userId":"16892616782940751714"}}},"outputs":[],"source":["import spacy\n","import pandas as pd\n","\n","# Load spaCy's English model\n","nlp = spacy.load(\"en_core_web_sm\")"]},{"cell_type":"code","source":["# Function to get the negation of a sentence\n","def negate_statement(sentence):\n","    sentence = sentence.replace(\".\", \"\")  # Remove the period for processing\n","    doc = nlp(sentence)\n","    negated_tokens = []\n","    negation_inserted = False  # To prevent multiple negations\n","\n","    for token in doc:\n","        # Handle frequency adverbs\n","        if token.tag_ == \"RB\" and token.text.lower() in {\"always\", \"often\", \"sometimes\"} and not negation_inserted:\n","            negated_tokens.append(\"do not\")\n","            negated_tokens.append(token.text)\n","            negation_inserted = True\n","\n","        # Special case for \"has\"\n","        elif token.text.lower() == \"has\" and not negation_inserted:\n","            if token.dep_ == \"ROOT\":\n","                negated_tokens.append(\"does not have\")\n","                negation_inserted = True\n","            else:\n","                negated_tokens.append(\"has not\")\n","                negation_inserted = True\n","        # Special case for auxiliary verbs\n","        elif token.text.lower() in {\"am\", \"is\", \"are\", \"has\", \"was\"} and not negation_inserted:\n","            negated_tokens.append(f\"{token.text} not\")\n","            negation_inserted = True\n","\n","        # Handle the root verb\n","        elif token.pos_ == \"VERB\" and token.dep_ == \"ROOT\" and not negation_inserted:\n","            lemma = token.lemma_  # Lemmatize to base form\n","            if token.tag_ == \"VBD\":  # Past tense\n","                negated_tokens.append(f\"did not {lemma}\")\n","            elif token.tag_ == \"VBZ\":  # Present tense singular\n","                negated_tokens.append(f\"does not {lemma}\")\n","            elif token.tag_ in {\"VBP\", \"VB\"}:  # Present tense plural or base form\n","                negated_tokens.append(f\"do not {lemma}\")\n","            negation_inserted = True\n","\n","        # Ensure auxiliary verbs modifying the root verb are included correctly\n","        elif token.dep_ in {\"aux\", \"auxpass\"} and not negation_inserted:\n","            negated_tokens.append(f\"{token.text} not\")\n","            negation_inserted = True\n","\n","        # Keep other tokens as they are\n","        else:\n","            negated_tokens.append(token.text)\n","\n","    # Join the tokens into the negated sentence\n","    negated_sentence = \" \".join(negated_tokens)\n","    return f\"{negated_sentence}.\"\n","\n"],"metadata":{"id":"4uH72_sKw-Jq","executionInfo":{"status":"ok","timestamp":1734048724861,"user_tz":480,"elapsed":228,"user":{"displayName":"Carson Chiem","userId":"16892616782940751714"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Function to see if connective is a contradiction connective\n","def is_contradiction(connective):\n","    connect_contradict = [\"and\", \"if\", \"so\", \"therefore\", \"but\", \"when\", \"although\"]\n","    if connective in connect_contradict:\n","        return 1\n","    else:\n","        return 0"],"metadata":{"id":"5TCFLqYwNcmB","executionInfo":{"status":"ok","timestamp":1734048728029,"user_tz":480,"elapsed":375,"user":{"displayName":"Carson Chiem","userId":"16892616782940751714"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Function to insert modals into given sentences\n","def modals(sentence, negation, connective):\n","    doc_s = nlp(sentence)\n","    doc_n = nlp(negation)\n","\n","    # Initialize subject and complement lists\n","    subject_s, complement_s = [], []\n","    subject_n, complement_n = [], []\n","\n","    # Map auxiliary verbs to their modal complements\n","    aux_modal_map = {\n","        \"have\": f\"{connective} have\",\n","        \"has\": f\"{connective} have\",\n","        \"do\": f\"{connective} do\",\n","        \"does\": f\"{connective} do\",\n","        \"did\": f\"{connective} do\",\n","        \"am\": f\"{connective} be\",\n","        \"is\": f\"{connective} be\",\n","        \"are\": f\"{connective} be\",\n","        \"was\": f\"{connective} be\",\n","        \"were\": f\"{connective} be\",\n","    }\n","\n","    aux_modal_neg_map = {\n","        \"have\": f\"{connective} not have\",\n","        \"has\": f\"{connective} not have\",\n","        \"do\": f\"{connective} not do\",\n","        \"does\": f\"{connective} not do\",\n","        \"did\": f\"{connective} not do\",\n","        \"am\": f\"{connective} not be\",\n","        \"is\": f\"{connective} not be\",\n","        \"are\": f\"{connective} not be\",\n","        \"was\": f\"{connective} not be\",\n","        \"were\": f\"{connective} not be\",\n","    }\n","\n","    aux_verbs_do = {\"do\", \"does\", \"did\"}\n","\n","    def process_doc(doc, is_negated):\n","        subject, complement = [], []\n","        modal_inserted = False\n","\n","        for token in doc:\n","            # Getting the subject\n","            if \"subj\" in token.dep_:\n","                # Collect the main subject and its modifiers\n","                subject_parts = [token]\n","                for child in token.children:\n","                    if child.dep_ in {\"det\", \"poss\", \"amod\", \"compound\"}:\n","                        subject_parts.append(child)\n","                        complement.remove(child.text)\n","\n","                # Sort modifiers and the main token by their position in the sentence\n","                subject_parts = sorted(subject_parts, key=lambda x: x.idx)\n","                subject.append(\" \".join([t.text for t in subject_parts]))\n","            elif token.dep_ == \"expl\":  # Existential \"There\"\n","                subject.append(token.text)\n","            # Checking next token is an auxiliary verb\n","            elif token.text.lower() in aux_modal_map and not modal_inserted:\n","                if is_negated:\n","                    # Special case for \"do\" verbs if it is not the root\n","                    if token.dep_ != \"ROOT\" and token.text.lower() in aux_verbs_do:\n","                        modal = f\"{connective} not\"\n","                    else:\n","                        modal = aux_modal_neg_map[token.text.lower()]\n","                else:\n","                    modal = aux_modal_map[token.text.lower()]\n","                complement.append(modal)\n","                modal_inserted = True\n","            # Modifying the root verbs\n","            elif token.dep_ == \"ROOT\" and not modal_inserted:\n","                lemma = token.lemma_\n","                if is_negated:\n","                    modal = f\"{connective} not {lemma}\"\n","                else:\n","                    modal = f\"{connective} {lemma}\"\n","                complement.append(modal)\n","                modal_inserted = True\n","            elif token.text.lower() == \"not\":\n","                continue  # Skip explicit \"not\" as it's already handled\n","            else:\n","                complement.append(token.text)\n","\n","        return \" \".join(subject), \" \".join(complement)\n","\n","    # Process positive and negated sentences\n","    subject_s, complement_s = process_doc(doc_s, is_negated=False)\n","    subject_n, complement_n = process_doc(doc_n, is_negated=True)\n","\n","    # Return the formatted output\n","    return f\"{subject_s} {complement_s} and {subject_n} {complement_n}\""],"metadata":{"id":"wEqJmZ39aeyM","executionInfo":{"status":"ok","timestamp":1734048729652,"user_tz":480,"elapsed":238,"user":{"displayName":"Carson Chiem","userId":"16892616782940751714"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Function to lowercase and capitalize sentences\n","def create_sentence(sentence, negation, connective):\n","    # Joining sentences\n","    if connective in connectives1:\n","        stimuli = f\"{sentence} {connective} {negation}\"\n","    elif connective in connectives2:\n","        stimuli = f\"{connective} {sentence}, {connective} {negation}\"\n","    elif connective in connectives3: # Calls the modal function to insert modal\n","        stimuli = modals(sentence, negation, connective)\n","    else:\n","        stimuli = f\"Either {sentence}, or {negation}\"\n","\n","    doc = nlp(stimuli)\n","    # Create a list to hold the transformed sentence\n","    new_sentence = [doc[0].text.capitalize()]  # First word gets capitalized\n","\n","    # Iterate through the tokens starting from the second word\n","    for token in doc[1:]:\n","        # If the token is a proper noun\n","        if token.pos_ in {'PROPN'} or token.text == \"I\":\n","            new_sentence.append(token.text)\n","        elif token.is_punct:\n","            if (token.text == \".\" and token.i == len(doc) - 1) or token.text != \".\":\n","                new_sentence[-1] += token.text\n","        else:\n","            new_sentence.append(token.text.lower())\n","\n","    # Join the words into a new sentence\n","    return f\"{' '.join(new_sentence)}\""],"metadata":{"id":"IK3haVUw8cTM","executionInfo":{"status":"ok","timestamp":1734048735134,"user_tz":480,"elapsed":475,"user":{"displayName":"Carson Chiem","userId":"16892616782940751714"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["connectives1 = [\"and\", \"if\", \"so\", \"therefore\", \"but\", \"when\", \"although\", \"or\"] # easy insert connectives\n","connectives2 = [\"maybe\", \"perhaps\"] # connectives before both sentences\n","connectives3 = [\"might\"] # modals inserted within sentences\n","connectives = connectives1 + connectives2 + connectives3 # all connectives"],"metadata":{"id":"ywcIQ77byP1P","executionInfo":{"status":"ok","timestamp":1734048738168,"user_tz":480,"elapsed":138,"user":{"displayName":"Carson Chiem","userId":"16892616782940751714"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Reading statements from CSV and outputting to CSV\n","csv_path = \"exp1_propositions.csv\" # replace with correct filename\n","fields = [\"Stimuli\"]\n","row_order = 1\n","\n","stimuli_df = pd.read_csv(csv_path, usecols = fields)\n","\n","csv_output = []\n","\n","for _, row in stimuli_df.iterrows():\n","    statement = row[\"Stimuli\"]\n","    for connective in connectives:\n","        p = statement\n","        not_p = negate_statement(statement)\n","        sentence = create_sentence(p, not_p, connective)\n","        contradiction = is_contradiction(connective)\n","        csv_output.append({\"Sentence\": sentence, \"Type\": \"experimental\", \"P\": p, \"Not P\": not_p, \"Contradiction\": contradiction, \"Connective\": connective})\n","\n","output_df = pd.DataFrame(csv_output)\n","output_df.to_csv(\"exp1_prompts.csv\", index=False)"],"metadata":{"id":"VSZEUJGIqv75","executionInfo":{"status":"ok","timestamp":1734048753363,"user_tz":480,"elapsed":5855,"user":{"displayName":"Carson Chiem","userId":"16892616782940751714"}}},"execution_count":16,"outputs":[]}]}
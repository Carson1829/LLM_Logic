{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Request Access\n",
    "Fill out the form provided by Meta AI (or the entity distributing the LLaMA weights) to request access to the model weights.\n",
    "\n",
    "Step 2: Download the Weights\n",
    "Once you have access, download the weights to a directory on your local machine.\n",
    "\n",
    "Step 3: Convert the Weights\n",
    "Use the provided conversion script to convert the weights to the Hugging Face format.\n",
    "\n",
    "Here is a detailed guide on how to perform the conversion:\n",
    "\n",
    "Prerequisites\n",
    "Make sure you have Python installed and the transformers library from Hugging Face.\n",
    "\n",
    "pip install transformers\n",
    "Conversion Script\n",
    "Download the Conversion Script:\n",
    "The script is part of the Hugging Face Transformers repository. You can find it in the src/transformers/models/llama directory.\n",
    "\n",
    "git clone https://github.com/huggingface/transformers\n",
    "cd transformers\n",
    "Run the Conversion Script:\n",
    "Replace /path/to/downloaded/llama/weights with the actual path where you downloaded the LLaMA weights and specify the model size (e.g., 7B).\n",
    "\n",
    "python src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n",
    "    --input_dir /path/to/downloaded/llama/weights \\\n",
    "    --model_size 7B \\\n",
    "    --output_dir /path/to/output/directory\n",
    "\n",
    "Example\n",
    "Suppose you downloaded the weights to /home/user/llama_weights and want to convert the 7B model. The command would look like this:\n",
    "python src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n",
    "    --input_dir /home/user/llama_weights \\\n",
    "    --model_size 7B \\\n",
    "    --output_dir /home/user/llama_hf\n",
    "\n",
    "After conversion, you can load the model and tokenizer from the specified output directory using the Hugging Face transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: llama-7b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_dir = \"huggingface/llama-7b\" #NOT THE ACTUAL DIRECTORY\n",
    "try:\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_dir)\n",
    "    model = LlamaForCausalLM.from_pretrained(model_dir)\n",
    "    print(\"model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV path (change accordingly)\n",
    "csv_path = \"lungu_scale_updated.csv\" \n",
    "prompts = []\n",
    "fields = ['Instructions', 'Condition', 'Study', 'Type', 'Proposition', 'NAND', 'NOR', 'Positive AND']  # Columns used in CSV\n",
    "row_order = 1\n",
    " \n",
    "# Reading CSV to dataframe\n",
    "prompts_df = pd.read_csv(csv_path, usecols=fields)\n",
    "for _, row in prompts_df.iterrows():\n",
    "    prompt = {}\n",
    "\n",
    "    if (row['Condition'] == \"Control\"):\n",
    "        text_parts = [row['Instructions'], row['Proposition']]\n",
    "    \n",
    "    else:\n",
    "        # Check if either \"NAND\" or \"NOR\" or \"AND\" exists in the row\n",
    "        choices = {\"NAND\":row[\"NAND\"], \"NOR\":row[\"NOR\"], \"Contradictory\":row[\"Positive AND\"]}\n",
    "        available_choices = {key: value for key, value in choices.items() if not pd.isna(value)}\n",
    "\n",
    "        if available_choices:\n",
    "            # Randomly choose one from available choices\n",
    "            chosen_column, content = random.choice(list(available_choices.items()))\n",
    "        else:\n",
    "            chosen_column, content = '', ''  # \"Control\" rows without continuations \"A\" or \"B\" or \"C\"\n",
    "\n",
    "        text_parts = [row['Instructions'], row['Proposition'], str(content)]\n",
    "        \n",
    "    # Filter out empty strings before joining\n",
    "    prompt[\"text\"] = ' '.join(filter(None, text_parts))\n",
    "    prompt[\"condition\"] = row['Condition']\n",
    "    prompt[\"study\"] = row['Study']\n",
    "    prompt[\"type\"] = row['Type']\n",
    "    if chosen_column:\n",
    "        prompt[\"choice\"] = chosen_column\n",
    "    else:\n",
    "        prompt[\"choice\"] = \"none\"\n",
    "    \n",
    "    prompt[\"order\"] = row_order\n",
    "    row_order += 1\n",
    "    prompts.append(prompt)\n",
    "\n",
    "random.shuffle(prompts)\n",
    "\n",
    "print(f\"Found {len(prompts)} prompts in the given CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "\n",
    "# Function to generate responses using the chosen model\n",
    "def generate_response(prompt_text):\n",
    "    system_message = \"If prompted to respond with a number on a scale, only respond with the whole number. For example, only respond with '6' if you believe it is 6 on the scale. Do not give any explanations. Interpret both sentences together, not separately.\"\n",
    "    user_message = prompt_text\n",
    "\n",
    "    # Combine system and user messages into a single string\n",
    "    combined_prompt = f\"{system_message}\\nUser: {user_message}\\n\"\n",
    "\n",
    "    inputs = tokenizer(combined_prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=50, num_return_sequences=1)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # response = text_generation_pipeline(combined_prompt, max_length=50, num_return_sequences=1)\n",
    "    return response # response[0]['generated_text']\n",
    "\n",
    "# Generate responses\n",
    "for prompt in prompts:\n",
    "    answer = generate_response(prompt[\"text\"])\n",
    "    outputs.append(answer)\n",
    "\n",
    "print(\"messages generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Prompt\": [prompt[\"text\"] for prompt in prompts], \n",
    "                   \"Continuation\": [prompt[\"choice\"] for prompt in prompts], \n",
    "                   \"Condition\": [prompt[\"condition\"] for prompt in prompts],\n",
    "                   \"Study\": [prompt[\"study\"] for prompt in prompts],\n",
    "                   \"Type\": [prompt[\"type\"] for prompt in prompts],\n",
    "                   \"Original order\": [prompt[\"order\"] for prompt in prompts],\n",
    "                   \"Response\": outputs})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv_path = \"output_responses_llama.csv\"\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Responses saved to {output_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

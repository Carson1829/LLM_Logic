{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1vduiYS4mKTXkD3Q_Fr4z3XZ8bEYCI0Fh","timestamp":1733961092215}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"t8FpaltAw89L","executionInfo":{"status":"ok","timestamp":1735868225418,"user_tz":480,"elapsed":1309,"user":{"displayName":"Carson Chiem","userId":"16892616782940751714"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f3517975-bfba-4bd5-c827-ecf745958cdd"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n","  warnings.warn(Warnings.W111)\n"]}],"source":["import spacy\n","import pandas as pd\n","import random\n","import csv\n","\n","# Load spaCy's English model\n","nlp = spacy.load(\"en_core_web_sm\")"]},{"cell_type":"code","source":["# Function to insert modals into given sentences\n","def modals(P, Q, P_type_bool, Q_type_bool, connective):\n","    doc_p = nlp(P)\n","    doc_q = nlp(Q)\n","\n","    # Initialize subject and complement lists\n","    subject_p, complement_p = [], []\n","    subject_q, complement_q = [], []\n","\n","    # Map auxiliary verbs to their modal complements\n","    aux_modal_map = {\n","        \"have\": f\"{connective} have\",\n","        \"has\": f\"{connective} have\",\n","        \"do\": f\"{connective} do\",\n","        \"does\": f\"{connective} do\",\n","        \"did\": f\"{connective} do\",\n","        \"am\": f\"{connective} be\",\n","        \"is\": f\"{connective} be\",\n","        \"are\": f\"{connective} be\",\n","        \"was\": f\"{connective} be\",\n","        \"were\": f\"{connective} be\",\n","    }\n","\n","    aux_modal_neg_map = {\n","        \"have\": f\"{connective} not have\",\n","        \"has\": f\"{connective} not have\",\n","        \"do\": f\"{connective} not do\",\n","        \"does\": f\"{connective} not do\",\n","        \"did\": f\"{connective} not do\",\n","        \"am\": f\"{connective} not be\",\n","        \"is\": f\"{connective} not be\",\n","        \"are\": f\"{connective} not be\",\n","        \"was\": f\"{connective} not be\",\n","        \"were\": f\"{connective} not be\",\n","    }\n","\n","    aux_verbs_do = {\"do\", \"does\", \"did\"}\n","\n","    def process_doc(doc, is_negated):\n","        subject, complement = [], []\n","        modal_inserted = False\n","\n","        for token in doc:\n","            # Getting the subject\n","            if \"subj\" in token.dep_:\n","                # Collect the main subject and its modifiers\n","                subject_parts = [token]\n","                for child in token.children:\n","                    if child.dep_ in {\"det\", \"poss\", \"amod\", \"compound\"}:\n","                        subject_parts.append(child)\n","                        complement.remove(child.text)\n","\n","                # Sort modifiers and the main token by their position in the sentence\n","                subject_parts = sorted(subject_parts, key=lambda x: x.idx)\n","                subject.append(\" \".join([t.text for t in subject_parts]))\n","            elif token.dep_ == \"expl\":  # Existential \"There\"\n","                subject.append(token.text)\n","            # Checking next token is an auxiliary verb\n","            elif token.text.lower() in aux_modal_map and not modal_inserted:\n","                if is_negated:\n","                    # Special case for \"do\" verbs if it is not the root\n","                    if token.dep_ != \"ROOT\" and token.text.lower() in aux_verbs_do:\n","                        modal = f\"{connective} not\"\n","                    else:\n","                        modal = aux_modal_neg_map[token.text.lower()]\n","                else:\n","                    modal = aux_modal_map[token.text.lower()]\n","                complement.append(modal)\n","                modal_inserted = True\n","            # Modifying the root verbs\n","            elif token.dep_ == \"ROOT\" and not modal_inserted:\n","                lemma = token.lemma_\n","                if is_negated:\n","                    modal = f\"{connective} not {lemma}\"\n","                else:\n","                    modal = f\"{connective} {lemma}\"\n","                complement.append(modal)\n","                modal_inserted = True\n","            elif token.text.lower() == \"not\":\n","                continue  # Skip explicit \"not\" as it's already handled\n","            else:\n","                complement.append(token.text)\n","\n","        return \" \".join(subject), \" \".join(complement)\n","\n","    # Process positive and negated sentences\n","    subject_p, complement_p = process_doc(doc_p, P_type_bool)\n","    subject_q, complement_q = process_doc(doc_q, Q_type_bool)\n","\n","    # Return the formatted output\n","    return f\"{subject_p} {complement_p} and {subject_q} {complement_q}\""],"metadata":{"id":"wEqJmZ39aeyM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to lowercase and capitalize sentences\n","def create_sentence(P, Q, connective, P_type_bool, Q_type_bool):\n","    # Joining sentences\n","    if connective in connectives1:\n","        stimuli = f\"{P} {connective} {Q}\"\n","    elif connective in connectives2:\n","        stimuli = f\"{connective} {P}, {connective} {Q}\"\n","    elif connective in connectives3: # Calls the modal function to insert modal\n","        stimuli = modals(P, Q, P_type_bool, Q_type_bool, connective)\n","    else:\n","        stimuli = f\"Either {P}, or {Q}\"\n","\n","    doc = nlp(stimuli)\n","    # Create a list to hold the transformed sentence\n","    new_sentence = [doc[0].text.capitalize()]  # First word gets capitalized\n","\n","    # Iterate through the tokens starting from the second word\n","    for token in doc[1:]:\n","        # If the token is a proper noun\n","        if token.pos_ in {'PROPN'} or token.text == \"I\":\n","            new_sentence.append(token.text)\n","        elif token.is_punct:\n","            if (token.text == \".\" and token.i == len(doc) - 1) or token.text != \".\":\n","                new_sentence[-1] += token.text\n","        else:\n","            new_sentence.append(token.text.lower())\n","\n","    # Join the words into a new sentence\n","    return f\"{' '.join(new_sentence)}\""],"metadata":{"id":"IK3haVUw8cTM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["connectives1 = [\"and\", \"if\", \"so\", \"therefore\", \"but\", \"when\", \"although\", \"or\"] # easy insert connectives\n","connectives2 = [\"maybe\", \"perhaps\"] # connectives before both sentences\n","connectives3 = [\"might\"] # modals inserted within sentences\n","connectives = connectives1 + connectives2 + connectives3 # all connectives"],"metadata":{"id":"ywcIQ77byP1P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reading statements from CSV and outputting to CSV\n","csv_path = \"exp1_prompts.csv\" # replace with correct filename\n","fields = [\"P\", \"Not P\"]\n","row_order = 1\n","\n","stimuli_df = pd.read_csv(csv_path, usecols = fields)\n","\n","with open('exp1_prompts.csv', 'a', newline='') as csvFile:\n","    writer = csv.writer(csvFile)\n","    for i in range(100): # Change for the number of control stimuli needed\n","        P_row = random.randint(0, len(stimuli_df) - 1)\n","        Q_row = random.randint(0, len(stimuli_df) - 1)\n","        while P_row == Q_row: # If they are the same\n","            Q_row = random.randint(0, len(stimuli_df) - 1)\n","\n","        P_type = random.choice([\"P\", \"Not P\"])\n","        Q_type = random.choice([\"P\", \"Not P\"])\n","        P = stimuli_df.iloc[P_row][P_type]\n","        Q = stimuli_df.iloc[Q_row][Q_type]\n","        # Checks to see if sentence is negated\n","        P_type_bool = P_type == \"Not P\"\n","        Q_type_bool = Q_type == \"Not P\"\n","\n","        connective = random.choice(connectives)\n","        sentence = create_sentence(P, Q, connective, P_type_bool, Q_type_bool)\n","        writer.writerow([sentence, \"control\", \"N/A\", \"N/A\", \"0\", connective])\n","csvFile.close()"],"metadata":{"id":"VSZEUJGIqv75"},"execution_count":null,"outputs":[]}]}